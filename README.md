# Introduction to Airflow Pipeline

Welcome to the repository that hosts code snippets discussed in the blog post titled [Crafting a Basic Data Pipeline Using Airflow](https://datacurious.hashnode.dev/crafting-a-basic-data-pipeline-with-airflow) The blog provides insights into building a straightforward data pipeline with Apache Airflow.

## Overview

The main focus of this repository is to illustrate the implementation of a scraping task. We leverage the power of Apache Airflow to orchestrate the entire process, from data scraping to loading.

## Getting Started

To get started, make sure you have Airflow set up on your local machine. If you haven't done so yet, you can follow the instructions provided [here](https://datacurious.hashnode.dev/streamlining-airflow-deployment-leveraging-docker-compose) to set up.

### Prerequisites

- beautifulsoup4
- pandas
- apache-airflow

Feel free to explore the provided code samples and adapt them to your own data pipeline needs. Happy coding!
